{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import DarkNews as dn\n",
    "\n",
    "from DarkNews import const\n",
    "\n",
    "from DarkNews.GenLauncher import GenLauncher\n",
    "\n",
    "from DarkNews import Cfourvec as Cfv\n",
    "\n",
    "from particle import literals as lp\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory tree for this run already exists. Overriding it.\n",
      "13504.557608335577\n"
     ]
    }
   ],
   "source": [
    "MODEL_KWARGS = {'HNLtype': 'dirac', 'UD4': 1.0, 'alphaD': 0.25, 'Umu4': np.sqrt(9e-7), 'epsilon': np.sqrt(2e-10/const.alphaQED)}\n",
    "\n",
    "gen = GenLauncher(mzprime=0.03, m4=0.420, neval=1000, exp=\"miniboone_fhc\", loglevel='warning', seed=42, numpy=True, parquet=True, sparse=False, **MODEL_KWARGS)\n",
    "df=gen.run(loglevel=\"warning\")\n",
    "\n",
    "print(df.w_event_rate.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'experiment': <DarkNews.detector.Detector object at 0x7fc188ac2e50>, 'model': <DarkNews.model.ThreePortalModel object at 0x7fc1b9132280>, 'data_path': PosixPath('data/miniboone_fhc/3plus1/m4_0.42_mzprime_0.03_dirac')}\n"
     ]
    }
   ],
   "source": [
    "gen_path = df.attrs['data_path']\n",
    "df_pq = pd.read_parquet(Path(f\"{gen_path}/pandas_df.parquet\"), engine='pyarrow')\n",
    "print(df_pq.attrs)\n",
    "df_pq = pd.read_pickle(Path(f\"{gen_path}/pandas_df.pckl\"))\n",
    "print(df_pq.attrs)\n",
    "\n",
    "nda = np.load(Path(f\"{gen_path}/ndarray.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df_pq\n",
    "# helicity only present in sparse format\n",
    "if 'helicity' in df_pandas.columns.levels[0]:\n",
    "    # test that numpy array and dataframe formats save the same information\n",
    "    df_pandas = df_pandas.replace(to_replace='conserving', value= '+1')\n",
    "    df_pandas = df_pandas.replace(to_replace='flipping',   value= '-1')\n",
    "\n",
    "# remove non-numeric entries\n",
    "drop_list = ['underlying_process','target','scattering_regime']\n",
    "if not set(drop_list).isdisjoint(df_pandas.columns.levels[0]):\n",
    "    df_for_numpy = df_pandas.drop([col for col in drop_list if col in df_pandas.columns.levels[0]], axis=1, level=0).to_numpy(dtype=np.float64) \n",
    "else:\n",
    "    df_for_numpy = df_pandas.to_numpy(dtype=np.float64)  \n",
    "assert (df_for_numpy[nda!=0]/nda[nda!=0]!=1).sum() == 0, 'pandas dataframe and numpy array seem to contain different data.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.17188653e-01,  9.16124727e-01,  1.59980230e-02, ...,\n",
       "         3.73399539e-06,  2.01213964e-15,  1.38903642e+01],\n",
       "       [ 1.37667063e+00,  1.37617485e+00,  2.59185364e-02, ...,\n",
       "         2.16799358e-06,  3.58706846e-15,  8.95753095e+00],\n",
       "       [ 1.29687705e+00,  1.29584745e+00, -2.41520320e-02, ...,\n",
       "         3.18334345e-06,  1.75033740e-15,  1.16851308e+01],\n",
       "       ...,\n",
       "       [ 8.43364289e-01,  8.40950749e-01,  1.11306121e-01, ...,\n",
       "         1.15874669e-06,  1.12970402e-15,  2.67871672e-04],\n",
       "       [ 1.09838309e+00,  1.05749783e+00, -2.30827745e-01, ...,\n",
       "         1.26884935e-06,  8.97558554e-16,  2.66636955e-03],\n",
       "       [ 6.74185894e-01,  6.71551468e-01, -1.37383525e-01, ...,\n",
       "         1.11720019e-06,  6.00265885e-16,  2.48204550e-04]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "drop_list = ['underlying_process','target','scattering_regime']\n",
    "\n",
    "if not set(drop_list).isdisjoint(df_pq.columns.levels[0]):\n",
    "    df_for_numpy = df_pq.drop([col for col in drop_list if col in df_pq.columns.levels[0]], axis=1, level=0).to_numpy(dtype=np.float64)\n",
    "else:\n",
    "    df_for_numpy = df_pq.to_numpy(dtype=np.float64)\n",
    "df_for_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-43c5b9fc6e2a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-43c5b9fc6e2a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df_for_numpy = df_pq.drop(,axis=1, level=0)\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " df_for_numpy = df_pq.drop(,axis=1, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.10'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import vegas \n",
    "import gvar\n",
    "gvar.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DarkNews import fourvec as fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the three-portal model.\n",
      "---------------------------------------------------------\n",
      "   ______           _        _   _                     \n",
      "   |  _  \\         | |      | \\ | |                    \n",
      "   | | | |__ _ _ __| | __   |  \\| | _____      _____   \n",
      "   | | | / _  | ___| |/ /   | .   |/ _ \\ \\ /\\ / / __|  \n",
      "   | |/ / (_| | |  |   <    | |\\  |  __/\\ V  V /\\__ \\  \n",
      "   |___/ \\__,_|_|  |_|\\_\\   \\_| \\_/\\___| \\_/\\_/ |___/  \n",
      "\n",
      "---------------------------------------------------------\n",
      "Model:\n",
      "\t1 dirac heavy neutrino(s).\n",
      "\n",
      "---------------------------------------------------------\n",
      "Experiment:\n",
      "\tMiniBooNE_FHC\n",
      "\tfluxfile loaded: ../fluxes/MiniBooNE_FHC.dat\n",
      "\tPOT: 1.875e+21\n",
      "\tnuclear targets: ['C12', 'H1']\n",
      "\tfiducial mass: [701.1428571428571, 116.85714285714286] tonnes\n",
      "\n",
      "Directory tree for this run already exists. Overriding it.\n"
     ]
    }
   ],
   "source": [
    "gen = GenLauncher(mzprime=0.03, m4=0.420, epsilon=epsilon_def, Umu4=umu4_def, UD4=ud4_def, gD=gD_def, \n",
    "                    neval=1000, HNLtype=\"dirac\", exp=\"miniboone_fhc\", loglevel='INFO',\n",
    "                    seed=333,\n",
    "                    parquet=True, numpy=True, hepevt=True, sparse=True, print_to_float32=True)\n",
    "\n",
    "df=gen.run(loglevel=\"ERROR\")\n",
    "df_2=gen.run(loglevel=\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_c(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun \n",
    "\n",
    "gen.dn_printer.print_events_to_HEPEVT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"./data/miniboone_fhc/3plus1/m4_0.42_mzprime_0.03_dirac/\"\n",
    "\n",
    "df_std = pd.read_pickle(f\"{filename}pandas_df.pckl\")\n",
    "df_pq = pd.read_parquet(f\"{filename}pandas_df.parquet\", engine='pyarrow')\n",
    "nda = np.load(f\"{filename}ndarray.npy\")\n",
    "# ndz = np.load(f\"{filename}ndarray.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nda_fix=np.reshape(nda,(np.shape(nda)[0],int(np.shape(nda)[1]/4),4)).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### npz format tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f'{v[0]}_{v[1]}' if v[1] else f'{v[0]}' for v in df.columns.values]\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that numpy array and dataframe formats save the same information\n",
    "assert (df_std.to_numpy()[nda!=0]/nda[nda!=0]!=1).sum() == 0 \n",
    "assert (df_pq.to_numpy()[nda!=0]/nda[nda!=0]!=1).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'as':\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vegas tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the VEGAS integration methods:\n",
    "import vegas as vg\n",
    "from collections import OrderedDict\n",
    "\n",
    "class test_integral(vg.BatchIntegrand):\n",
    "\n",
    "    def __init__(self, dim, c = 1):\n",
    "        self.dim = dim\t\n",
    "        self.c = c\n",
    "        self.analytical_func = lambda x: np.exp(self.c * x) * (self.c*x-1)/self.c**2\n",
    "        self.analytical_res  = (self.analytical_func(2) - self.analytical_func(0))\n",
    "\n",
    "        # initialize\n",
    "        self.norm = {'diff_xsec': 1, 'diff_xsec2': 1}\n",
    "        # normalize integrand with an initial throw\n",
    "        _throw = self.__call__(np.random.rand(dim,500), np.ones((dim,500)))\n",
    "        for key,val in _throw.items():\n",
    "            self.norm[key] = np.mean(val)\n",
    "    \n",
    "    def __call__(self, x, jac):\n",
    "\n",
    "        xmax = 2\n",
    "        xmin = 0\n",
    "        xx=np.empty(0)\n",
    "        diff = 1\n",
    "        diff2 = 1\n",
    "        for d in range(self.dim):\n",
    "            xx = x[:,d]*(xmax - xmin) + xmin\n",
    "            diff *= np.exp(self.c*xx)*xx\n",
    "            if d < self.dim - 2:\n",
    "                diff2 *= np.exp(self.c*xx)*xx\n",
    "\n",
    "        # hypercube jacobian (vegas hypercube --> physical limits) transformation\n",
    "        hypercube_jacobian = (xmax - xmin)\n",
    "        diff  *= hypercube_jacobian**self.dim\n",
    "        diff2 *= hypercube_jacobian**(self.dim)\n",
    "\n",
    "        ##############################################\n",
    "        # return all differential quantities of interest\n",
    "        self.int_dic = OrderedDict()\t\t\n",
    "        self.int_dic['diff_xsec'] = diff\n",
    "        self.int_dic['diff_xsec2'] = diff2\n",
    "        \n",
    "        ##############################################\n",
    "        # storing normalization for integrands to be of O(1) numbers\t\t\n",
    "        # normalization\n",
    "        for key in self.int_dic:\n",
    "            self.int_dic[key] /= self.norm[key]\n",
    "        self.int_dic['diff_xsec2'] /= jac[:,-1]*jac[:,-2]\n",
    "        return self.int_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = []\n",
    "evals2 = []\n",
    "for i in range(5):\n",
    "    #########################################################################\n",
    "    # BATCH SAMPLE INTEGRAND OF INTEREST\n",
    "    DIM = 6\n",
    "    batch_f = test_integral(dim=DIM)\n",
    "    integ = vg.Integrator(DIM*[[0.0, 1.0]]) # unit hypercube\n",
    "    result = dn.MC.run_vegas(batch_f, integ, NINT=20, NEVAL=10000, NINT_warmup=20, NEVAL_warmup=1000)\n",
    "    ##########################################################################\n",
    "    # print(result.summary())\n",
    "    evals.append(result['diff_xsec'].mean*batch_f.norm['diff_xsec'])\n",
    "    evals2.append(result['diff_xsec2'].mean*batch_f.norm['diff_xsec2'])\n",
    "\n",
    "plt.plot(evals/evals[0], color='blue')\n",
    "plt.plot(evals/batch_f.analytical_res**DIM, ls='--', color='blue')\n",
    "plt.plot(evals2/evals2[0], color='orange')\n",
    "plt.plot(evals2/batch_f.analytical_res**(DIM-2)/4, ls='--', color='orange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, weights, jac = dn.MC.get_samples(integ, batch_f, return_jac=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(weights['diff_xsec'])*batch_f.norm['diff_xsec']/batch_f.analytical_res**DIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(weights['diff_xsec'])*batch_f.norm['diff_xsec']/batch_f.analytical_res**(DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,vals in weights.items():\n",
    "    plt.plot(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights['diff_xsec2']*jac[:,5]).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 20\n",
    "\n",
    "_ = plt.hist(samples[0,:]*2, weights=weights['diff_xsec2']*batch_f.norm['diff_xsec2']/(2/bins), bins=bins, density=False)\n",
    "\n",
    "x=np.linspace(0,2)\n",
    "\n",
    "plt.plot(x, np.exp(batch_f.c * x)*x * batch_f.analytical_res**(DIM-3)*4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# profile amplitude calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proton = dn.detector.NuclearTarget(\"H1\")\n",
    "bsm_model = dn.model.create_3portal_HNL_model(mzprime=0.1, m4 = 0.01, Umu4=1e-3)\n",
    "calculator = dn.MC.XsecCalc(bsm_model = bsm_model, scattering_regime = 'p-el', nuclear_target= proton, helicity = 'conserving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = np.ones(1000)\n",
    "%prun dn.amplitudes.upscattering_dxsec_dQ2([one,one,one], calculator.ups_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# profile full generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud4_def = 1.0\n",
    "alphaD = 0.25\n",
    "gD_def = np.sqrt(alphaD*4*np.pi)\n",
    "umu4_def = np.sqrt(9e-7)\n",
    "ud4 = 1.\n",
    "epsilon_def = np.sqrt(2e-10/const.alphaQED)\n",
    "\n",
    "gen = GenLauncher(mzprime=0.03, m4=0.420, epsilon=epsilon_def, Umu4=umu4_def, UD4=ud4_def, gD=gD_def, neval=1000, HNLtype=\"dirac\", exp=\"miniboone_fhc\", loglevel='INFO')\n",
    "gen.run(log=\"INFO\")\n",
    "df_mini = gen.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun gen.run(log=\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mini.w_event_rate.sum()*0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = GenLauncher(mzprime=0.03, m4=0.420, epsilon=epsilon_def, Umu4=umu4_def, UD4=ud4_def, gD=gD_def, neval=1000, HNLtype=\"dirac\", exp=\"microboone\", loglevel='ERROR')\n",
    "gen.run(log=\"ERROR\")\n",
    "df_micro = gen.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud4_def = 1.0\n",
    "alphaD = 0.4\n",
    "gD_def = np.sqrt(alphaD*4*np.pi)\n",
    "umu4_def = 4e-4\n",
    "umu5_def = 4e-4\n",
    "ud4 = 1.\n",
    "ud5 = 1.\n",
    "epsilon_def = 2.2e-2\n",
    "\n",
    "gen = GenLauncher(mzprime=1.25, m4=0.010, m5=0.490, epsilon=epsilon_def, Umu4=umu4_def, Umu5=umu5_def, UD4=ud4_def, UD5=ud5, gD=gD_def, neval=10000, HNLtype=\"majorana\", exp=\"miniboone_fhc\", loglevel='ERROR')\n",
    "gen.run(log=\"ERROR\")\n",
    "df_mini = gen.df\n",
    "\n",
    "gen = GenLauncher(mzprime=1.25, m4=0.010, m5=0.490, epsilon=epsilon_def, Umu4=umu4_def, Umu5=umu5_def, UD4=ud4_def, UD5=ud5, gD=gD_def, neval=10000, HNLtype=\"majorana\", exp=\"microboone\", loglevel='ERROR')\n",
    "gen.run(log=\"ERROR\")\n",
    "df_micro = gen.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ud4_def = 1.0\n",
    "alphaD = 0.4\n",
    "gD_def = np.sqrt(alphaD*4*np.pi)\n",
    "umu4_def = 4e-4\n",
    "umu5_def = 4e-4\n",
    "ud4 = 1.\n",
    "ud5 = 1.\n",
    "epsilon_def = 2.2e-2\n",
    "\n",
    "gen = GenLauncher(m4=0.10, epsilon=0.0, dmu4= 1e-6, gD=0.0, neval=1000, HNLtype=\"dirac\", exp=\"miniboone_fhc\", loglevel='ERROR', decay_product='photon')\n",
    "gen.run(log=\"ERROR\")\n",
    "df_mini = gen.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mini['P_decay_N_parent'][['1','2','3']].iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" {dn.printer.print_in_order(df_mini.iat[0,'P_projectile'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun -l 5\n",
    "\n",
    "gen3p1 = GenLauncher(hepevt=False, m4=0.420, epsilon=0.0, mu_tr_mu4= 1e-6, nopelastic=False, gD=0.0, neval=10000, HNLtype=\"dirac\", exp=\"miniboone_fhc\", loglevel='ERROR', decay_products='photon')\n",
    "# gen3p2 = GenLauncher(hepevt=False, m4=0.420, epsilon=0.0, mu_tr_mu4= 1e-6, nopelastic=False, gD=0.0, neval=10000, HNLtype=\"majorana\", exp=\"miniboone_fhc\", loglevel='ERROR', decay_products='photon')\n",
    "gen3p2 = GenLauncher(hepevt=False, m4=0.370, m5=0.420, epsilon=0.0, mu_tr_mu5= 1e-6, mu_tr_45= 1e-4, nopelastic=False, gD=0.0, neval=10000, HNLtype=\"dirac\", exp=\"miniboone_fhc\", loglevel='ERROR', decay_products='photon')\n",
    "gen3p1.run(log=\"ERROR\")\n",
    "gen3p2.run(log=\"ERROR\")\n",
    "df_3p1 = gen3p1.df\n",
    "df_3p2 = gen3p2.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy\n",
    "\n",
    "bins = np.linspace(0,2,20)\n",
    "df = df_3p1\n",
    "mask = (df.scattering_regime=='p-el')\n",
    "x=df.P_decay_gamma.to_numpy()[:,0]\n",
    "w=df.w_event_rate.to_numpy()\n",
    "_ = plt.hist(x, weights=w, bins=bins, histtype='step', color='dodgerblue', ls='-')\n",
    "_ = plt.hist(x[mask], weights=w[mask], bins=bins, histtype='step', color='orange', ls='-')\n",
    "\n",
    "df = df_3p2\n",
    "mask = (df.scattering_regime=='p-el')\n",
    "x=df.P_decay_gamma.to_numpy()[:,0]\n",
    "w=df.w_event_rate.to_numpy()\n",
    "_ = plt.hist(x, weights=w, bins=bins, histtype='step', color='dodgerblue', ls='--')\n",
    "_ = plt.hist(x[mask], weights=w[mask], bins=bins, histtype='step', color='orange', ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle\n",
    "\n",
    "bins = np.linspace(-1,1,20)\n",
    "df = df_3p1\n",
    "mask = (df.scattering_regime=='p-el')\n",
    "x=dn.Cfv.get_cosTheta(df.P_decay_gamma.to_numpy())\n",
    "w=df.w_event_rate.to_numpy()\n",
    "_ = plt.hist(x, weights=w, bins=bins, histtype='step',color='dodgerblue', ls='-')\n",
    "_ = plt.hist(x[mask], weights=w[mask], histtype='step',bins=bins, color='orange', ls='-')\n",
    "\n",
    "df = df_3p2\n",
    "mask = (df.scattering_regime=='p-el')\n",
    "x=dn.Cfv.get_cosTheta(df.P_decay_gamma.to_numpy())\n",
    "w=df.w_event_rate.to_numpy()\n",
    "_ = plt.hist(x, weights=w, bins=bins, histtype='step', color='dodgerblue', ls='--')\n",
    "_ = plt.hist(x[mask], weights=w[mask], bins=bins, histtype='step', color='orange', ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"event rate Mini {df_micro['w_event_rate'].sum()*0.05}\")\n",
    "print(f\"event rate Micro {df_micro['w_event_rate'].sum()*0.05*87/818*(550/470)**2*6/18.75}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Cfv.get_cosTheta((df_mini['P_decay_ell_minus']+df_mini['P_decay_ell_plus']).to_numpy())\n",
    "w = df_mini['w_event_rate'].to_numpy()\n",
    "_ = plt.hist(x, weights=w, bins=np.linspace(-1,1,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"event rate Mini {df_mini['w_event_rate'].sum()*0.05}\")\n",
    "print(f\"event rate Micro rescaled {df_mini['w_event_rate'].sum()*87/818*(550/470)**2*6/18.75}\")\n",
    "print(f\"event rate Micro {df_micro['w_event_rate'].sum()*0.5}\")\n",
    "print(f\"event rate SBND {df_micro['w_event_rate'].sum()*112/87*(470/110)**2*0.5}\")\n",
    "print(f\"event rate Icarus {df_micro['w_event_rate'].sum()*476/87*(470/600)**2*0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DarkNews import Cfourvec as Cfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Cfv.get_cosTheta(df_micro['P_decay_ell_minus'].to_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud4_def = 1.0\n",
    "alphaD = 0.25\n",
    "gD_def = np.sqrt(alphaD*4*np.pi)\n",
    "umu4_def = np.sqrt(9e-8)\n",
    "ud4 = 1.\n",
    "epsilon_def = np.sqrt(2e-10/const.alphaQED)\n",
    "\n",
    "gen = GenLauncher(mzprime=0.03, m4=0.100, epsilon=epsilon_def, Umu4=umu4_def, UD4=ud4_def, gD=gD_def, neval=10000, HNLtype=\"dirac\", exp=\"miniboone_fhc\", loglevel='ERROR')\n",
    "gen.run(log=\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_m = GenLauncher(mzprime=0.03, m4=0.100, epsilon=epsilon_def, Umu4=umu4_def, UD4=ud4_def, gD=gD_def, neval=10000, HNLtype=\"majorana\", exp=\"miniboone_fhc\", loglevel='ERROR')\n",
    "gen_m.run(log=\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gen.df\n",
    "df_m = gen_m.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['w_event_rate'].sum()*0.047)\n",
    "print(df_m['w_event_rate'].sum()*0.047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(0,2):\n",
    "    gen_object = GenLauncher(m4 = 0.100, mzprime = 0.03, neval = 10000, nitn=20)\n",
    "    gen_object.run(log=\"INFO\")\n",
    "    df_1 = gen_object.df\n",
    "\n",
    "    # gen_object = GenLauncher(m4 = 0.100, mzprime = 0.02, neval = 1000, nint=20)\n",
    "    gen_object.run(log=\"INFO\")\n",
    "    df_2 = gen_object.df\n",
    "\n",
    "    df.append([df_1, df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ratios_rate = []\n",
    "ratios_fxsec = []\n",
    "ratios_decay = []\n",
    "for pair in df:\n",
    "    ratios_rate.append(np.sum(pair[0]['w_event_rate'])/np.sum(pair[1]['w_event_rate']))\n",
    "    ratios_fxsec.append(np.sum(pair[0]['w_flux_avg_xsec'])/np.sum(pair[1]['w_flux_avg_xsec']))\n",
    "    \n",
    "    ratios_decay.append(np.sum(pair[0]['w_decay_rate_0'])/np.sum(pair[1]['w_decay_rate_0']))\n",
    "\n",
    "plt.scatter(range(2),ratios_rate, label=\"rate\")\n",
    "plt.scatter(range(2),ratios_fxsec, label=\"fxsec\")\n",
    "plt.scatter(range(2),ratios_decay, label=\"decay\")\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "# plt.scatter(range(0,2),case1, c='blue')\n",
    "# plt.scatter(range(0,2),case2, c='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_1['w_flux_avg_xsec'])\n",
    "# plt.yscale(\"log\")\n",
    "# plt.ylim(1,1e7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1,df2 = df[0]\n",
    "p1 = (df1['P_projectile'] - df1['P_decay_N_parent']).to_numpy()\n",
    "h1 = dn.Cfourvec.dot4(p1,p1)\n",
    "\n",
    "p2 = (df2['P_projectile'] - df2['P_decay_N_parent']).to_numpy()\n",
    "h2 = dn.Cfourvec.dot4(p2,p2)\n",
    "\n",
    "_ = plt.hist(np.sqrt(-h1), bins=100, range=(0,0.5), histtype='step', weights=df1['w_event_rate'], lw=1)\n",
    "_ = plt.hist(np.sqrt(-h2), bins=100, range=(0,0.5), histtype='step', weights=df2['w_event_rate'], lw=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = (df_2['P_projectile']).to_numpy()\n",
    "# p1 = (df_1['P_decay_ell_minus']+df_1['P_decay_ell_plus']).to_numpy()\n",
    "# p1 = (df_1['P_decay_ell_minus']+df_1['P_decay_ell_plus']+df_1['P_decay_N_daughter']).to_numpy()\n",
    "h1 = p1[:,0]\n",
    "_ = plt.hist(h1, bins=30, histtype='step', range=(0,2), weights=df_2['w_flux_avg_xsec'],density=True, label='True neutrino energy')\n",
    "plt.xlabel(\"Enu\")\n",
    "# plt.yscale(\"log\")\n",
    "\n",
    "\n",
    "x = np.linspace(0,2, 1000)\n",
    "y = df_1.attrs['experiment'].FLUX_FUNCTIONS[1](x)/( df_1.attrs['experiment'].FLUX_FUNCTIONS[1](x)*(x[1]-x[0])).sum()\n",
    "\n",
    "plt.plot(x,y, label='Input flux')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p1 = (df_2['P_decay_ell_minus']+df_2['P_decay_ell_plus']).to_numpy()\n",
    "\n",
    "h1 = p1[:,0]\n",
    "_ = plt.hist(h1, bins=30, histtype='step', range=(0,2), weights=df_2['w_event_rate'],density=True, label='e+e- energy')\n",
    "plt.xlabel(\"e+ e- total energy\")\n",
    "# plt.yscale(\"log\")\n",
    "\n",
    "\n",
    "x = np.linspace(0,2, 1000)\n",
    "y = df_1.attrs['experiment'].FLUX_FUNCTIONS[1](x)/( df_1.attrs['experiment'].FLUX_FUNCTIONS[1](x)*(x[1]-x[0])).sum()\n",
    "\n",
    "plt.plot(x,y, label='Input flux')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = (df_1['P_projectile']).to_numpy()\n",
    "x = p1[:,0]\n",
    "y  = (df_1['P_decay_N_parent']).to_numpy()[:,2]\n",
    "_ = plt.scatter(x, y, marker='.', s=3,  label='True neutrino energy')\n",
    "plt.xlabel(\"Enu\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6869619afde5ccaa692f7f4d174735a0f86b1f7ceee086952855511b0b6edec0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
